{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyM7tBhVASHTzvNSKzTooGMo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rupeshthapa123/NotebookProject/blob/main/MidTerm_NLP_RupeshThapa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Midterm Exam\n",
        "\n",
        "Importing all the necessary libraries such as tokenizers"
      ],
      "metadata": {
        "id": "uez560EdxsaK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFJA3L11kAbW",
        "outputId": "e7658d8b-e87a-4fed-8855-047baf48385c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.23.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the tokenizers module\n",
        "import tokenizers"
      ],
      "metadata": {
        "id": "0i936K6JkNmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Tokenizer1 with model BPE and unk_token=[UNK]"
      ],
      "metadata": {
        "id": "936JCVADyKJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Tokenizer class from the tokenizers module\n",
        "from tokenizers import Tokenizer\n",
        "\n",
        "# Import the BPE model from the tokenizers.models module\n",
        "from tokenizers.models import BPE\n",
        "\n",
        "# Create a tokenizer using the BPE model and specify the unknown token to be '[UNK]'\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))"
      ],
      "metadata": {
        "id": "nR9UYG-hkNjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a BpeTrainer object and assign the argument special_tokens=[“[UNK]”,“[CLS]”, “[SEP]”, “[PAD]”, “[MASK]”]"
      ],
      "metadata": {
        "id": "oqdhxQd7yU-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the BpeTrainer class from the tokenizers module\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "# Create an instance of the BpeTrainer class\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[PAD]\",\"[MASK]\"])"
      ],
      "metadata": {
        "id": "9BLvaV8XkNgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a list of files for test, train and validation following the name convention in the dataset downloaded from the provided link.\n",
        "Train the tokenizer using the list of files and the trainer object."
      ],
      "metadata": {
        "id": "f4An4gL_yiVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of file paths for the training, testing, and validation data\n",
        "files = [\n",
        "    # Create a string with the file path for the training data\n",
        "    f\"sample_data/wiki.{split}.raw\" for split in [\"test\",\"train\",\"valid\"]\n",
        "]\n",
        "# Use the tokenizer to train on the files, using the trainer\n",
        "tokenizer.train(files, trainer)"
      ],
      "metadata": {
        "id": "uyzhD3qTkNdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a list of 5 strings to be processed by the tokenizer"
      ],
      "metadata": {
        "id": "2b-SSLF9y47Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example texts to encode\n",
        "texts = [\n",
        "    \"This is the first text test for model.\",\n",
        "    \"Here is another one testing content.\",\n",
        "    \"And yet another text or sentence for encoding .\",\n",
        "    \"A lab exam or midterm for NLP.\",\n",
        "    \"Rainy day with a heat warning issued for a week.\"\n",
        "]"
      ],
      "metadata": {
        "id": "Aiw8cHi7kNbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use output=tokenizer.encode_batch function to encode the list"
      ],
      "metadata": {
        "id": "RwVBidbEzelg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_texts = tokenizer.encode_batch(texts)"
      ],
      "metadata": {
        "id": "XVrR3s7fkNYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the encoded outputs\n",
        "for i, encoded in enumerate(encoded_texts):\n",
        "    print(f\"Text {i+1}:\")\n",
        "    print(f\"  Tokens: {encoded.tokens}\")\n",
        "    print(f\"  IDs: {encoded.ids}\")\n",
        "    print(f\"  Attention Mask: {encoded.attention_mask}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WffATXMLkNV5",
        "outputId": "40b56d2b-aaf0-4684-e01c-f9c61f6cd679"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text 1:\n",
            "  Tokens: ['This ', 'is the first ', 'text ', 'test ', 'for ', 'model', '.']\n",
            "  IDs: [10807, 17525, 11540, 8703, 5104, 13263, 20]\n",
            "  Attention Mask: [1, 1, 1, 1, 1, 1, 1]\n",
            "\n",
            "Text 2:\n",
            "  Tokens: ['Here ', 'is ', 'another ', 'one ', 'testing ', 'cont', 'ent', '.']\n",
            "  IDs: [22852, 5059, 6199, 5242, 19234, 5296, 5119, 20]\n",
            "  Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n",
            "Text 3:\n",
            "  Tokens: ['And ', 'yet ', 'another ', 'text ', 'or ', 'sentence ', 'for ', 'enc', 'od', 'ing ', '.']\n",
            "  IDs: [20210, 8651, 6199, 11540, 5133, 20241, 5104, 5445, 5232, 5042, 20]\n",
            "  Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n",
            "Text 4:\n",
            "  Tokens: ['A ', 'lab', ' ', 'ex', 'am ', 'or ', 'mid', 'term ', 'for ', 'N', 'LP', '.']\n",
            "  IDs: [5575, 7175, 6, 5146, 5553, 5133, 9146, 7923, 5104, 52, 26439, 20]\n",
            "  Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n",
            "Text 5:\n",
            "  Tokens: ['Rain', 'y ', 'day ', 'with a ', 'heat ', 'warning ', 'issu', 'ed for a ', 'week', '.']\n",
            "  IDs: [16838, 5024, 5693, 5659, 12862, 12738, 6283, 11390, 8322, 20]\n",
            "  Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using a different model and trainer WordLevel model and WordLevelTrainer"
      ],
      "metadata": {
        "id": "-WhLfE0BznV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the WordLevel model from the tokenizers.models module\n",
        "from tokenizers.models import WordLevel\n",
        "\n",
        "# Create a tokenizer using the WordLevel model and specify the unknown token to be '[UNK]'\n",
        "tokenizer2 = Tokenizer(WordLevel(unk_token=\"[UNK]\"))"
      ],
      "metadata": {
        "id": "oEzONbIZkNTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the BpeTrainer class from the tokenizers module\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "\n",
        "# Create an instance of the BpeTrainer class\n",
        "trainer2 = WordLevelTrainer(special_tokens=[\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[PAD]\",\"[MASK]\"])"
      ],
      "metadata": {
        "id": "WwUlaW26mNjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of file paths for the training, testing, and validation data\n",
        "files = [\n",
        "    # Create a string with the file path for the training data\n",
        "    f\"sample_data/wiki.{split}.raw\" for split in [\"test\",\"train\",\"valid\"]\n",
        "]\n",
        "\n",
        "# Use the tokenizer to train on the files, using the trainer\n",
        "tokenizer2.train(files, trainer2)"
      ],
      "metadata": {
        "id": "LjF7-QmNmNhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example texts to encode\n",
        "texts = [\n",
        "    \"This is the first text test for model.\",\n",
        "    \"Here is another one testing content.\",\n",
        "    \"And yet another text or sentence for encoding .\",\n",
        "    \"A lab exam or midterm for NLP.\",\n",
        "    \"Rainy day with a heat warning issued for a week.\"\n",
        "]"
      ],
      "metadata": {
        "id": "uPri2rb-mNer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_texts2 = tokenizer2.encode_batch(texts)"
      ],
      "metadata": {
        "id": "OO8oLRRzmNb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the encoded outputs\n",
        "for i, encoded in enumerate(encoded_texts2):\n",
        "    print(f\"Text {i+1}:\")\n",
        "    print(f\"  Tokens: {encoded.tokens}\")\n",
        "    print(f\"  IDs: {encoded.ids}\")\n",
        "    print(f\"  Attention Mask: {encoded.attention_mask}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljkvnGAImNZG",
        "outputId": "835ef876-71c4-4872-c38d-5a741a14f760"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text 1:\n",
            "  Tokens: ['[UNK]']\n",
            "  IDs: [0]\n",
            "  Attention Mask: [1]\n",
            "\n",
            "Text 2:\n",
            "  Tokens: ['[UNK]']\n",
            "  IDs: [0]\n",
            "  Attention Mask: [1]\n",
            "\n",
            "Text 3:\n",
            "  Tokens: ['[UNK]']\n",
            "  IDs: [0]\n",
            "  Attention Mask: [1]\n",
            "\n",
            "Text 4:\n",
            "  Tokens: ['[UNK]']\n",
            "  IDs: [0]\n",
            "  Attention Mask: [1]\n",
            "\n",
            "Text 5:\n",
            "  Tokens: ['[UNK]']\n",
            "  IDs: [0]\n",
            "  Attention Mask: [1]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using a different model and trainer WordPiece model and WordPieceTrainer\n"
      ],
      "metadata": {
        "id": "voSV-6lqzt9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the WordLevel model from the tokenizers.models module\n",
        "from tokenizers.models import WordPiece\n",
        "\n",
        "# Create a tokenizer using the WordLevel model and specify the unknown token to be '[UNK]'\n",
        "tokenizer3 = Tokenizer(WordPiece(unk_token=\"[UNK]\"))"
      ],
      "metadata": {
        "id": "14YfxVQDmNWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the BpeTrainer class from the tokenizers module\n",
        "from tokenizers.trainers import WordPieceTrainer\n",
        "\n",
        "# Create an instance of the BpeTrainer class\n",
        "trainer3 = WordPieceTrainer(special_tokens=[\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[PAD]\",\"[MASK]\"])"
      ],
      "metadata": {
        "id": "xmcoAzv0mNTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of file paths for the training, testing, and validation data\n",
        "files = [\n",
        "    # Create a string with the file path for the training data\n",
        "    f\"sample_data/wiki.{split}.raw\" for split in [\"test\",\"train\",\"valid\"]\n",
        "]\n",
        "\n",
        "# Use the tokenizer to train on the files, using the trainer\n",
        "tokenizer3.train(files, trainer3)"
      ],
      "metadata": {
        "id": "rAOk8XENpAH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example texts to encode\n",
        "texts = [\n",
        "    \"This is the first text test for model.\",\n",
        "    \"Here is another one testing content.\",\n",
        "    \"And yet another text or sentence for encoding .\",\n",
        "    \"A lab exam or midterm for NLP.\",\n",
        "    \"Rainy day with a heat warning issued for a week.\"\n",
        "]"
      ],
      "metadata": {
        "id": "EPhpPyxEpABM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_texts3 = tokenizer3.encode_batch(texts)"
      ],
      "metadata": {
        "id": "c9KEcsf_pQOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the encoded outputs\n",
        "for i, encoded in enumerate(encoded_texts3):\n",
        "    print(f\"Text {i+1}:\")\n",
        "    print(f\"  Tokens: {encoded.tokens}\")\n",
        "    print(f\"  IDs: {encoded.ids}\")\n",
        "    print(f\"  Attention Mask: {encoded.attention_mask}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpTlg05RpVUi",
        "outputId": "0f87c7ac-522c-40c8-a80e-8c45131c03b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text 1:\n",
            "  Tokens: ['T', '##his ', '##is the first ', '##text ', '##test ', '##for ', '##model', '##.']\n",
            "  IDs: [58, 10140, 23388, 16552, 13721, 10111, 18423, 5040]\n",
            "  Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n",
            "Text 2:\n",
            "  Tokens: ['H', '##ere ', '##is ', '##another ', '##one ', '##testing ', '##cont', '##ent', '##.']\n",
            "  IDs: [46, 10133, 10067, 11204, 10250, 24263, 10305, 10127, 5040]\n",
            "  Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n",
            "Text 3:\n",
            "  Tokens: ['A', '##nd ', '##yet ', '##another ', '##text ', '##or ', '##sentence ', '##for ', '##enc', '##odi', '##ng ', '##.']\n",
            "  IDs: [39, 12552, 13663, 11204, 16552, 10141, 25282, 10111, 10453, 16279, 28671, 5040]\n",
            "  Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n",
            "Text 4:\n",
            "  Tokens: ['A', '## ', '##lab', '## ', '##exam', '## ', '##or ', '##mid', '##term ', '##for ', '##N', '##L', '##P', '##.']\n",
            "  IDs: [39, 5018, 12193, 5018, 11691, 5018, 10141, 14189, 12946, 10111, 5072, 5056, 5049, 5040]\n",
            "  Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n",
            "Text 5:\n",
            "  Tokens: ['R', '##ain', '##y ', '##day ', '##with a ', '##heat ', '##warning ', '##issued ', '##for a ', '##week', '##.']\n",
            "  IDs: [56, 10200, 10032, 10700, 10667, 17882, 17763, 19953, 11093, 13340, 5040]\n",
            "  Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From the above models that were used for tokenizing, the WordLevel model and Trainer didnot perform well as it could not tokenize the dataset but WordPiece model and Trainer were able to tokenize the dataset but did it based on the grammar. BPE model and trainer would be the best as it can tokenize the large datasets correctly without based on grammar alone.**"
      ],
      "metadata": {
        "id": "5KiWlLKKqM8k"
      }
    }
  ]
}