{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMS9/CdIfWSeQ6/t6UOPSTR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rupeshthapa123/NotebookProject/blob/main/RupeshThapa_Lab4_Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Tokenization\n",
        "\n",
        "Tokenizer: Functionally, a tokenizer reads text and outputs a mapping\n",
        "between words and indices. It obtain a dictionary mapping words to numbers.Also, considers words, punctuation, grammar and complex structures. Such dictionary can be fed into an NLP model.\n",
        "\n",
        "In this lab we are hugging face Tokenizer.\n",
        "Tokenizer Components are as follows:\n",
        "\n",
        "• Normalizer: Executes all the initial transformations over the initial\n",
        "input string.\n",
        "\n",
        "• Pre-Tokenizer: Splits the initial input string.\n",
        "\n",
        "• Model: Handles sub token discovery and generation. Trainable.\n",
        "\n",
        "• Post-Processor: Deals with compatibility with other models and\n",
        "libraries.\n",
        "\n",
        "• Decode: Maps a tokenized input back into the original string.\n",
        "\n",
        "• Trainer: Training capabilities for each model."
      ],
      "metadata": {
        "id": "4_XkFCqvpVys"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ST1qk9t5m8PW"
      },
      "outputs": [],
      "source": [
        "#!pip install tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the tokenizers module\n",
        "import tokenizers"
      ],
      "metadata": {
        "id": "zSCrW1KvnXh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using an established tokenizer such as Byte-Pair Encoding (BPE) which is an implementation of subword tokenization method."
      ],
      "metadata": {
        "id": "CgIsvXR4rb64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Tokenizer class from the tokenizers module\n",
        "from tokenizers import Tokenizer\n",
        "\n",
        "# Import the BPE model from the tokenizers.models module\n",
        "from tokenizers.models import BPE\n",
        "\n",
        "# Create a tokenizer using the BPE model and specify the unknown token to be '[UNK]'\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))"
      ],
      "metadata": {
        "id": "iyrYJqXgnf0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize a special BpeTrainer object"
      ],
      "metadata": {
        "id": "CZ52OzW_rsGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the BpeTrainer class from the tokenizers module\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "# Create an instance of the BpeTrainer class\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[PAD]\",\"[MASK]\"])"
      ],
      "metadata": {
        "id": "ZKgwaTJ2n2ZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train a Model\n",
        "\n",
        "After downloading the wikitext-103-raw-v1 dataset from HuggingFace website  specify the files and train the model"
      ],
      "metadata": {
        "id": "o5ZgBQYpseAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of file paths for the training, testing, and validation data\n",
        "files = [\n",
        "    # Create a string with the file path for the training data\n",
        "    f\"sample_data/wiki.{split}.raw\" for split in [\"test\",\"train\",\"valid\"]\n",
        "]\n",
        "\n",
        "# Use the tokenizer to train on the files, using the trainer\n",
        "tokenizer.train(files, trainer)"
      ],
      "metadata": {
        "id": "djhSoEnMoOTe",
        "outputId": "5b0a43bf-4d75-4d2f-d476-a82797876321",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "No such file or directory (os error 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ce2fcc5b12e7>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Use the tokenizer to train on the files, using the trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mException\u001b[0m: No such file or directory (os error 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example texts to encode\n",
        "texts = [\n",
        "    \"This is the first text.\",\n",
        "    \"Here is another one.\",\n",
        "    \"And yet another text for encoding.\"\n",
        "]"
      ],
      "metadata": {
        "id": "DgwU6Fpy9n3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_texts = tokenizer.encode_batch(texts)"
      ],
      "metadata": {
        "id": "ed6CW09f6i_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the encoded outputs\n",
        "for i, encoded in enumerate(encoded_texts):\n",
        "    print(f\"Text {i+1}:\")\n",
        "    print(f\"  Tokens: {encoded.tokens}\")\n",
        "    print(f\"  IDs: {encoded.ids}\")\n",
        "    print(f\"  Attention Mask: {encoded.attention_mask}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prcdxqWY-wun",
        "outputId": "cf70e745-197e-4c08-a397-f4fd9be51804"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text 1:\n",
            "  Tokens: ['This ', 'is the first ', 'text', '.']\n",
            "  IDs: [2450, 17277, 16723, 20]\n",
            "  Attention Mask: [1, 1, 1, 1]\n",
            "\n",
            "Text 2:\n",
            "  Tokens: ['H', 'ere ', 'is another ', 'on', 'e.']\n",
            "  IDs: [46, 623, 15638, 548, 9069]\n",
            "  Attention Mask: [1, 1, 1, 1, 1]\n",
            "\n",
            "Text 3:\n",
            "  Tokens: ['And ', 'yet ', 'another ', 'text ', 'for ', 'en', 'co', 'd', 'ing', '.']\n",
            "  IDs: [15804, 4619, 1781, 9090, 628, 550, 1012, 71, 729, 20]\n",
            "  Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}